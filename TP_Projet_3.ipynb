{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20421/20421 [00:00<00:00, 805915.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "data = open(\"CISI.ALLnettoye\",\"r\").readlines()\n",
    "data=[i[:len(i)-1] for i in data] #Enleve le '\\n'\n",
    "\n",
    "collection = {}\n",
    "vocabulary={}\n",
    "titles=[]\n",
    "for line in tqdm(range(len(data))):\n",
    "    if data[line][:3]==\".I \":\n",
    "        doc=[]\n",
    "        vocab=[]\n",
    "    elif data[line-1][:3]==\".I \":\n",
    "        indice = int(data[line-1][3:])\n",
    "        titre=\"Document_\"+str(indice)+\"-> \"+str(data[line])\n",
    "        titles.append(titre)\n",
    "        vocab.append(data[line])\n",
    "        vocabulary[indice] = vocab\n",
    "    else:\n",
    "        doc.append(data[line])\n",
    "        collection[titre]=doc\n",
    "        vocab.append(data[line])\n",
    "        vocabulary[indice] = vocab\n",
    "\n",
    "v = {}\n",
    "for i in vocabulary.keys():\n",
    "    s = []\n",
    "    for j in vocabulary[i]:\n",
    "        wt = word_tokenize(j)\n",
    "        for w in wt:\n",
    "            s.append(w.lower())\n",
    "    v[i]=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pour = {}\n",
    "for key,val in vocabulary.items():\n",
    "    pour[key]=\" \".join(val)\n",
    "pour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tino\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "         00  000  029  044  054  071  071182  073  077  084  ...  zambia  \\\n",
      "Doc1      0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc2      0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc3      0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc4      0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc5      0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "...      ..  ...  ...  ...  ...  ...     ...  ...  ...  ...  ...     ...   \n",
      "Doc1456   0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc1457   0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc1458   0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc1459   0    0    0    0    0    0       0    0    0    0  ...       0   \n",
      "Doc1460   0    1    0    0    0    0       0    0    0    0  ...       0   \n",
      "\n",
      "         zero  zhurnal  ziman  zipf  zipfian  zone  zones  zoology  zuckerman  \n",
      "Doc1        0        0      0     0        0     0      0        0          0  \n",
      "Doc2        0        0      0     0        0     0      0        0          0  \n",
      "Doc3        0        0      0     0        0     0      0        0          0  \n",
      "Doc4        0        0      0     0        0     0      0        0          0  \n",
      "Doc5        0        0      0     0        0     0      0        0          0  \n",
      "...       ...      ...    ...   ...      ...   ...    ...      ...        ...  \n",
      "Doc1456     0        0      0     0        0     0      0        0          0  \n",
      "Doc1457     0        0      0     0        0     0      0        0          0  \n",
      "Doc1458     0        0      0     0        0     0      0        0          0  \n",
      "Doc1459     0        0      0     0        0     0      0        0          0  \n",
      "Doc1460     0        0      0     0        0     0      0        0          0  \n",
      "\n",
      "[1460 rows x 9712 columns]\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer \n",
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import pandas as pd\n",
    "# set of documents\n",
    "train = [val for val in pour.values()]\n",
    "#test = ['The sun in the sky is bright', 'We can see the zoology sun, the bright sun.']\n",
    "# instantiate the vectorizer object\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "# convert th documents into a matrix\n",
    "count_wm = countvectorizer.fit_transform(train)\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(train)\n",
    "#retrieve the terms found in the corpora\n",
    "# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n",
    "#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc'+str(i) for i in pour.keys()],columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc'+str(i) for i in pour.keys()],columns = tfidf_tokens)\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "df_tfidfvect.to_excel(\"test.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a24a58780a1ab0d9b51d25bf43a5dd07805581eb93440edbb86bdf625b0c7d7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
