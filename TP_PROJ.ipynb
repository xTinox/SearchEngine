{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP_PROJ - Moteur de recherche\n",
    "#### IMBROGNO Tino - CADOT Firmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"CISI.ALLnettoye\",\"r\").readlines()\n",
    "data=[i[:len(i)-1] for i in data] #Enleve le '\\n' de chaque ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La collection réunit l'ensemble des documents dans un dictionnaire avec pour clé: le titre du document | valeur: liste des lignes liées au contenu du document\n",
    "\n",
    "Les document sont un dictionnaire ayant comme clé: le numéro de document | valeur: titre + contenu\n",
    "\n",
    "Les titres sont contenus dans un dictionnaire ayant pour clé: le numéro du document | valeur: le titre du document associé  \n",
    "\n",
    "Les contenus (clé:numéro document | valeur:contenu du doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection={}\n",
    "document={}\n",
    "titre={}\n",
    "contenu={}\n",
    "\n",
    "for line in range(len(data)):\n",
    "    if data[line][:3]==\".I \":\n",
    "        doc=[]\n",
    "        content=[]\n",
    "        numDoc = int(data[line][3:])\n",
    "    elif data[line-1][:3]==\".I \":\n",
    "        titre[numDoc]=data[line]\n",
    "        doc.append(data[line])\n",
    "    else:\n",
    "        content.append(data[line])\n",
    "        doc.append(data[line])\n",
    "        contenu[numDoc]=content\n",
    "        document[numDoc]=doc\n",
    "        collection[titre[numDoc]]=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Editions of the Dewey Decimal Classifications \n",
      "\n",
      "['   The present study is a history of the DEWEY Decimal', 'Classification.  The first edition of the DDC was published', 'in 1876, the eighteenth edition in 1971, and future editions', \"will continue to appear as needed.  In spite of the DDC's\", 'long and healthy life, however, its full story has never', 'been told.  There have been biographies of Dewey', 'that briefly describe his system, but this is the first', 'attempt to provide a detailed history of the work that', 'more than any other has spurred the growth of', 'librarianship in this country and abroad.'] \n",
      "\n",
      "['18 Editions of the Dewey Decimal Classifications', '   The present study is a history of the DEWEY Decimal', 'Classification.  The first edition of the DDC was published', 'in 1876, the eighteenth edition in 1971, and future editions', \"will continue to appear as needed.  In spite of the DDC's\", 'long and healthy life, however, its full story has never', 'been told.  There have been biographies of Dewey', 'that briefly describe his system, but this is the first', 'attempt to provide a detailed history of the work that', 'more than any other has spurred the growth of', 'librarianship in this country and abroad.'] \n",
      "\n",
      "('18 Editions of the Dewey Decimal Classifications', ['   The present study is a history of the DEWEY Decimal', 'Classification.  The first edition of the DDC was published', 'in 1876, the eighteenth edition in 1971, and future editions', \"will continue to appear as needed.  In spite of the DDC's\", 'long and healthy life, however, its full story has never', 'been told.  There have been biographies of Dewey', 'that briefly describe his system, but this is the first', 'attempt to provide a detailed history of the work that', 'more than any other has spurred the growth of', 'librarianship in this country and abroad.']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(titre[1],'\\n')\n",
    "print(contenu[1],'\\n')\n",
    "print(document[1],'\\n')\n",
    "print(list(collection.items())[0],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordOccurrence(dc: dict):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    print(\"Loading for Word Occurrence\")\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    d=[[word_tokenize(line) for line in dc[i]] for i in dc.keys()] #Tokenization of all line of all documents in the collection\n",
    "    wordOccurrenceDoc = {}\n",
    "    for doc in tqdm(range(len(d))):\n",
    "        voc = []\n",
    "        wordOccurrenceDoc[doc+1]=dict()\n",
    "        for line in d[doc]:\n",
    "            for word in line:\n",
    "                wordLow = word.lower()\n",
    "                if len(wordLow)>1 and wordLow not in stopwords:\n",
    "                    voc.append(wordLow)\n",
    "                    wordOccurrenceDoc[doc+1][wordLow]=0\n",
    "        vocabulary.append(voc)\n",
    "\n",
    "    for doc in tqdm(range(len(d))):\n",
    "        for line in d[doc]:\n",
    "            for word in line:\n",
    "                wordLow = word.lower()\n",
    "                if wordLow in wordOccurrenceDoc[doc+1].keys():\n",
    "                    wordOccurrenceDoc[doc+1][wordLow] += 1\n",
    "    \n",
    "    return wordOccurrenceDoc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d[i] : List of lines from the document \"i\"\n",
    "### d[i][j] : List of word from the line \"j\" of the document \"i\"\n",
    "### d[i][j][k] : Word in place \"k\" from the line \"j\" of the document \"i\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def TF(doc,maxi):\n",
    "    print(\"Loading for TF\")\n",
    "    # maxi : Nb de docs analysés (maximum = N)\n",
    "    tfD={}\n",
    "    for i in tqdm(range(1,maxi+1)):\n",
    "        tfDict = {}\n",
    "        bagOfWordsCount = len(doc[i].keys())\n",
    "        for word, count in doc[i].items():\n",
    "            tfDict[word] = float(\"{:.15f}\".format(count / float(bagOfWordsCount)))\n",
    "        tfD[i] = tfDict\n",
    "    return tfD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "inverseDoc = {}\n",
    "def IDF(docu,wOccu,maxi):\n",
    "    print(\"Loading for IDF\")\n",
    "    # maxi : Nb de docs analysés (maximum = N)\n",
    "    d=[[word_tokenize(line) for line in docu[i]] for i in docu.keys()] #Tokenization of all line of all documents in the collection\n",
    "    N = len(wOccu)\n",
    "    sac_of_words={}\n",
    "    words={}\n",
    "    for i in tqdm(range(1,maxi+1)):\n",
    "        wordDict={}\n",
    "        for w in wOccu[i].keys():\n",
    "            if w in sac_of_words.keys():\n",
    "                wordDict[w] = sac_of_words[w]\n",
    "            else:\n",
    "                wordDict[w] = 0\n",
    "                inverseDoc[w] = []\n",
    "                for doc in d[:maxi]:\n",
    "                    isPresent=0\n",
    "                    for line in doc:\n",
    "                        for word in line:\n",
    "                            if w==word.lower():\n",
    "                                isPresent=1\n",
    "                                wordDict[w] += 1\n",
    "                                inverseDoc[w].append(d.index(doc)+1)\n",
    "                            if isPresent:\n",
    "                                break\n",
    "                        if isPresent:\n",
    "                            break\n",
    "                wordDict[w] = math.log(maxi/wordDict[w])\n",
    "                sac_of_words[w] = wordDict[w]\n",
    "        words[i]=wordDict\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(tfD, idfD):\n",
    "    print(\"Loading for TF-IDF\")\n",
    "    maxi = len(tfD)\n",
    "    if maxi != len(idfD):\n",
    "        print(\"Pas le même nombre de doc\")\n",
    "    else:\n",
    "        maxi += 1\n",
    "        tfidf = {}\n",
    "        for i in range(1,maxi):\n",
    "            tfidf_inter = {}\n",
    "            for word, val in tfD[i].items():\n",
    "                tfidf_inter[word] = val * idfD[i][word]\n",
    "            tfidf[i] = tfidf_inter\n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDict(dict):\n",
    "    dictionary={}\n",
    "    for i in dict.keys():\n",
    "        dico={}\n",
    "        for k, v in sorted(dict[i].items(), key=lambda x: -x[1]):\n",
    "            dico[k] = v\n",
    "        dictionary[i] = dico\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for Word Occurrence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1461/1461 [00:00<00:00, 2188.30it/s]\n",
      "100%|██████████| 1461/1461 [00:00<00:00, 13511.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1461/1461 [00:00<00:00, 12806.30it/s]\n"
     ]
    }
   ],
   "source": [
    "wO = wordOccurrence(document)\n",
    "tf = TF(wO,len(wO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for IDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1461 [00:18<39:46,  1.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tino\\Documents\\INSA\\Semestre_6\\TAL\\TP_Projet\\TP_Projet\\DATA\\TP_PROJ.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000014?line=0'>1</a>\u001b[0m idf \u001b[39m=\u001b[39m IDF(document,wO,\u001b[39mlen\u001b[39;49m(wO))\n",
      "\u001b[1;32mc:\\Users\\Tino\\Documents\\INSA\\Semestre_6\\TAL\\TP_Projet\\TP_Projet\\DATA\\TP_PROJ.ipynb Cell 11'\u001b[0m in \u001b[0;36mIDF\u001b[1;34m(docu, wOccu, maxi)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000010?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m doc:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000010?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m line:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000010?line=23'>24</a>\u001b[0m         \u001b[39mif\u001b[39;00m w\u001b[39m==\u001b[39mword\u001b[39m.\u001b[39;49mlower():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000010?line=24'>25</a>\u001b[0m             isPresent\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000010?line=25'>26</a>\u001b[0m             wordDict[w] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idf = IDF(document,wO,len(wO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for TF-IDF\n",
      "Pas le même nombre de doc\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tino\\Documents\\INSA\\Semestre_6\\TAL\\TP_Projet\\TP_Projet\\DATA\\TP_PROJ.ipynb Cell 16'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000015?line=0'>1</a>\u001b[0m tfidf \u001b[39m=\u001b[39m sortDict(TFIDF(tf,idf))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000015?line=1'>2</a>\u001b[0m tfidf[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdewey\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\Tino\\Documents\\INSA\\Semestre_6\\TAL\\TP_Projet\\TP_Projet\\DATA\\TP_PROJ.ipynb Cell 13'\u001b[0m in \u001b[0;36msortDict\u001b[1;34m(dict)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msortDict\u001b[39m(\u001b[39mdict\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000012?line=1'>2</a>\u001b[0m     dictionary\u001b[39m=\u001b[39m{}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000012?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mdict\u001b[39;49m\u001b[39m.\u001b[39;49mkeys():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000012?line=3'>4</a>\u001b[0m         dico\u001b[39m=\u001b[39m{}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000012?line=4'>5</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mdict\u001b[39m[i]\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39m-\u001b[39mx[\u001b[39m1\u001b[39m]):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "tfidf = sortDict(TFIDF(tf,idf))\n",
    "tfidf[1]['dewey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'large'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tino\\Documents\\INSA\\Semestre_6\\TAL\\TP_Projet\\TP_Projet\\DATA\\TP_PROJ.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000016?line=4'>5</a>\u001b[0m     inverseDoc[key]\u001b[39m=\u001b[39mdico_intermediaire\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000016?line=5'>6</a>\u001b[0m inverseDoc \u001b[39m=\u001b[39m sortDict(inverseDoc)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tino/Documents/INSA/Semestre_6/TAL/TP_Projet/TP_Projet/DATA/TP_PROJ.ipynb#ch0000016?line=6'>7</a>\u001b[0m inverseDoc[\u001b[39m\"\u001b[39;49m\u001b[39mlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'large'"
     ]
    }
   ],
   "source": [
    "for key in inverseDoc.keys():\n",
    "    dico_intermediaire={}\n",
    "    for doc in inverseDoc[key]:\n",
    "        dico_intermediaire[doc]=tfidf[doc][key]\n",
    "    inverseDoc[key]=dico_intermediaire\n",
    "inverseDoc = sortDict(inverseDoc)\n",
    "inverseDoc[\"large\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['essai', 'test', 'essai', 'try', 'hard', 'bonjour', 'france']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[1460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gensim\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "commonDict = gensim.corpora.Dictionary(vocabulary)\n",
    "\n",
    "corp = [commonDict.doc2bow(text) for text in vocabulary]\n",
    "\n",
    "lda = gensim.models.LdaMulticore(corp, num_topics)\n",
    "\n",
    "#lda_model = gensim.models.Doc2Vec(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edition'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0][corp[0][17][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWord(self):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    return [w.translate(str.maketrans('','',string.punctuation)) for w in self if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i+1]) for i, doc in enumerate(vocabulary)]\n",
    "model = Doc2Vec(documents, vector_size=1000, window=1, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(documents,epochs = 100,total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['librarians', 'decision', 'include', 'book', 'librarycollection', 'act', 'book', 'selection', 'censorship', 'there', 'fact', 'discernible', 'difference', 'two', 'terms', 'book', 'selection', 'andcensorship', 'topic', 'discussed', 'lucidly', 'ably', 'long', 'ago', 'bylester', 'asheim', 'become', 'classic', 'essay', 'literature', 'oflibrarianship', 'censorship', 'selection', 'raising', 'againmay', 'appear', 'exercise', 'redundancy']\n"
     ]
    }
   ],
   "source": [
    "recherche = \"Computerized information systems in fields related to chemistry.\"\n",
    "tokens = removeStopWord(recherche.lower().split())\n",
    "print(tokens)\n",
    "new_vector = model.infer_vector(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(238, 0.7939483523368835),\n",
       " (651, 0.5160901546478271),\n",
       " (306, 0.4982931911945343),\n",
       " (33, 0.48379889130592346),\n",
       " (865, 0.4549260139465332),\n",
       " (1440, 0.4510200321674347),\n",
       " (234, 0.4472435712814331),\n",
       " (939, 0.4373336136341095),\n",
       " (280, 0.4372183382511139),\n",
       " (370, 0.43611615896224976)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv.most_similar([new_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(inverseDoc)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ff37e48d066a05a946e815722dd6458ceed983ed1293fa257407e771528a86f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
